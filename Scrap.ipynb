{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da2de03-980d-4eba-ba4f-f133c5c17eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25436801-d844-453d-9c05-952b47d68573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c078d26d-e231-4562-85d3-66815e43ee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3066b323-d911-46f5-8c9d-b218f4e594ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Welcome! I'm glad you're here and excited to chat with you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# To give you a preview -- calling OpenAI with these messages is this easy. Any problems, head over to the Troubleshooting notebook.\n",
    "\n",
    "message = \"Hello, GPT! This is my first ever message to you! Hi!\"\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\", \"content\":message}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54794e2-bfae-4b20-b9d4-495f5a801b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffa9e969-c403-4b06-a61f-73defb592c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero to One: Learning Agentic Patterns\n",
      "Philschmid\n",
      "Search\n",
      "⌘\n",
      "k\n",
      "Blog\n",
      "Projects\n",
      "Newsletter\n",
      "About Me\n",
      "Toggle Menu\n",
      "Zero to One: Learning Agentic Patterns\n",
      "May 5, 2025\n",
      "16\n",
      "minute read\n",
      "View Code\n",
      "AI agents. Agentic AI. Agentic architectures. Agentic workflows. Agentic patterns. Agents are everywhere. But what exactly\n",
      "are\n",
      "they, and how do we build robust and effective agentic systems? While the term \"agent\" is used broadly, a key characteristic is their ability to dynamically plan and execute tasks, often leveraging external tools and memory to achieve complex goals.\n",
      "This post aims to explore common design patterns. Think of these patterns as blueprints or reusable templates for building AI applications. Understanding them provides a mental model for tackling complex problems and designing systems that are scalable, modular, and adaptable.\n",
      "We'll dive into several common patterns, differentiating between more structured\n",
      "workflows\n",
      "and more dynamic\n",
      "agentic patterns\n",
      ". Workflows typically follow predefined paths, while agents have more autonomy in deciding their course of action.\n",
      "Why Do (Agentic) Patterns Matter?\n",
      "Patterns provide a structured way to think and design systems.\n",
      "Patterns allow us to build and grow AI applications in complexity and adapt to changing requirements. Modular designs based on patterns are easier to modify and extend.\n",
      "Patterns help manage the complexity of coordinating multiple agents, tools, and workflows by offering proven, reusable templates. They promote best practices and shared understanding among developers.\n",
      "When (and When Not) to Use Agents?\n",
      "Before diving into patterns, it's crucial to consider\n",
      "when\n",
      "an agentic approach is truly necessary.\n",
      "Always seek the simplest solution first. If you know the exact steps required to solve a problem, a fixed workflow or even a simple script might be more efficient and reliable than a agent.\n",
      "Agentic systems often trade increased latency and computational cost for potentially better performance on complex, ambiguous, or dynamic tasks. Be sure the benefits outweigh these costs.\n",
      "Use\n",
      "workflows\n",
      "for predictability and consistency when dealing with well-defined tasks where the steps are known.\n",
      "Use\n",
      "agents\n",
      "when flexibility, adaptability, and model-driven decision-making are needed.\n",
      "Keep it Simple (Still): Even when building agentic systems, strive for the simplest effective design. Overly complex agent can become difficult to debug and manage.\n",
      "Agency introduces inherent unpredictability and potential errors. Agentic systems must incorporate robust error logging, exception handling, and retry mechanisms, allowing the system (or the underlying LLM) a chance to self-correct.\n",
      "Below, we'll explore 3 common workflow patterns and 4 agentic patterns. We'll illustrate each using pure API calls, without relying on specific frameworks like LangChain, LangGraph, LlamaIndex, or CrewAI, to focus on the core concepts.\n",
      "Pattern Overview\n",
      "We will cover the following patterns:\n",
      "Pattern Overview\n",
      "Workflow: Prompt Chaining\n",
      "Workflow: Routing or Handoff\n",
      "Workflow: Parallelization\n",
      "Reflection Pattern\n",
      "Tool Use Pattern\n",
      "Planning Pattern (Orchestrator-Workers)\n",
      "Multi-Agent Pattern\n",
      "Workflow: Prompt Chaining\n",
      "The output of one LLM call sequentially feeds into the input of the next LLM call. This pattern decomposes a task into a fixed sequence of steps. Each step is handled by an LLM call that processes the output from the preceding one. It's suitable for tasks that can be cleanly broken down into predictable, sequential subtasks.\n",
      "Use Cases:\n",
      "Generating a structured document: LLM 1 creates an outline, LLM 2 validates the outline against criteria, LLM 3 writes the content based on the validated outline.\n",
      "Multi-step data processing: Extracting information, transforming it, and then summarizing it.\n",
      "Generating newsletters based on curated inputs.\n",
      "import\n",
      "os\n",
      "from\n",
      "google\n",
      "import\n",
      "genai\n",
      "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
      "client\n",
      "=\n",
      "genai.Client(\n",
      "api_key\n",
      "=\n",
      "os.environ[\n",
      "\"GEMINI_API_KEY\"\n",
      "])\n",
      "# --- Step 1: Summarize Text ---\n",
      "original_text\n",
      "=\n",
      "\"Large language models are powerful AI systems trained on vast amounts of text data. They can generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\"\n",
      "prompt1\n",
      "=\n",
      "f\n",
      "\"Summarize the following text in one sentence:\n",
      "{\n",
      "original_text\n",
      "}\n",
      "\"\n",
      "# Use client.models.generate_content\n",
      "response1\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.0-flash'\n",
      ",\n",
      "contents\n",
      "=\n",
      "prompt1\n",
      ")\n",
      "summary\n",
      "=\n",
      "response1.text.strip()\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Summary:\n",
      "{\n",
      "summary\n",
      "}\n",
      "\"\n",
      ")\n",
      "# --- Step 2: Translate the Summary ---\n",
      "prompt2\n",
      "=\n",
      "f\n",
      "\"Translate the following summary into French, only return the translation, no other text:\n",
      "{\n",
      "summary\n",
      "}\n",
      "\"\n",
      "# Use client.models.generate_content\n",
      "response2\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.0-flash'\n",
      ",\n",
      "contents\n",
      "=\n",
      "prompt2\n",
      ")\n",
      "translation\n",
      "=\n",
      "response2.text.strip()\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Translation:\n",
      "{\n",
      "translation\n",
      "}\n",
      "\"\n",
      ")\n",
      "Workflow: Routing\n",
      "An initial LLM acts as a router, classifying the user's input and directing it to the most appropriate specialized task or LLM. This pattern implements a separation of concerns and allows for optimizing individual downstream tasks (using specialized prompts, different models, or specific tools) in isolation. It improves efficiency and potentially reduces costs by using smaller models for simpler tasks. When a task is routed, the selected agent \"takes over\" responsibility for completion.\n",
      "Use Cases:\n",
      "Customer support systems: Routing queries to agents specialized in billing, technical support, or product information.\n",
      "Tiered LLM usage: Routing simple queries to faster, cheaper models (like Llama 3.1 8B) and complex or unusual questions to more capable models (like Gemini 1.5 Pro).\n",
      "Content generation: Routing requests for blog posts, social media updates, or ad copy to different specialized prompts/models.\n",
      "import\n",
      "os\n",
      "import\n",
      "json\n",
      "from\n",
      "google\n",
      "import\n",
      "genai\n",
      "from\n",
      "pydantic\n",
      "import\n",
      "BaseModel\n",
      "import\n",
      "enum\n",
      "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
      "client\n",
      "=\n",
      "genai.Client(\n",
      "api_key\n",
      "=\n",
      "os.environ[\n",
      "\"GEMINI_API_KEY\"\n",
      "])\n",
      "# Define Routing Schema\n",
      "class\n",
      "Category\n",
      "(\n",
      "enum\n",
      ".\n",
      "Enum\n",
      "):\n",
      "WEATHER\n",
      "=\n",
      "\"weather\"\n",
      "SCIENCE\n",
      "=\n",
      "\"science\"\n",
      "UNKNOWN\n",
      "=\n",
      "\"unknown\"\n",
      "class\n",
      "RoutingDecision\n",
      "(\n",
      "BaseModel\n",
      "):\n",
      "category: Category\n",
      "reasoning:\n",
      "str\n",
      "# Step 1: Route the Query\n",
      "user_query\n",
      "=\n",
      "\"What's the weather like in Paris?\"\n",
      "# user_query = \"Explain quantum physics simply.\"\n",
      "# user_query = \"What is the capital of France?\"\n",
      "prompt_router\n",
      "=\n",
      "f\n",
      "\"\"\"\n",
      "Analyze the user query below and determine its category.\n",
      "Categories:\n",
      "- weather: For questions about weather conditions.\n",
      "- science: For questions about science.\n",
      "- unknown: If the category is unclear.\n",
      "Query:\n",
      "{\n",
      "user_query\n",
      "}\n",
      "\"\"\"\n",
      "# Use client.models.generate_content with config for structured output\n",
      "response_router\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.0-flash-lite'\n",
      ",\n",
      "contents\n",
      "=\n",
      "prompt_router,\n",
      "config\n",
      "=\n",
      "{\n",
      "'response_mime_type'\n",
      ":\n",
      "'application/json'\n",
      ",\n",
      "'response_schema'\n",
      ": RoutingDecision,\n",
      "},\n",
      ")\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Routing Decision: Category=\n",
      "{\n",
      "response_router.parsed.category\n",
      "}\n",
      ", Reasoning=\n",
      "{\n",
      "response_router.parsed.reasoning\n",
      "}\n",
      "\"\n",
      ")\n",
      "# Step 2: Handoff based on Routing\n",
      "final_response\n",
      "=\n",
      "\"\"\n",
      "if\n",
      "response_router.parsed.category\n",
      "==\n",
      "Category.\n",
      "WEATHER\n",
      ":\n",
      "weather_prompt\n",
      "=\n",
      "f\n",
      "\"Provide a brief weather forecast for the location mentioned in: '\n",
      "{\n",
      "user_query\n",
      "}\n",
      "'\"\n",
      "weather_response\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.0-flash'\n",
      ",\n",
      "contents\n",
      "=\n",
      "weather_prompt\n",
      ")\n",
      "final_response\n",
      "=\n",
      "weather_response.text\n",
      "elif\n",
      "response_router.parsed.category\n",
      "==\n",
      "Category.\n",
      "SCIENCE\n",
      ":\n",
      "science_response\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "\"gemini-2.5-flash-preview-04-17\"\n",
      ",\n",
      "contents\n",
      "=\n",
      "user_query\n",
      ")\n",
      "final_response\n",
      "=\n",
      "science_response.text\n",
      "else\n",
      ":\n",
      "unknown_response\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "\"gemini-2.0-flash-lite\"\n",
      ",\n",
      "contents\n",
      "=\n",
      "f\n",
      "\"The user query is:\n",
      "{\n",
      "prompt_router\n",
      "}\n",
      ", but could not be answered. Here is the reasoning:\n",
      "{\n",
      "response_router.parsed.reasoning\n",
      "}\n",
      ". Write a helpful response to the user for him to try again.\"\n",
      ")\n",
      "final_response\n",
      "=\n",
      "unknown_response.text\n",
      "print\n",
      "(\n",
      "f\n",
      "\"\n",
      "\\n\n",
      "Final Response:\n",
      "{\n",
      "final_response\n",
      "}\n",
      "\"\n",
      ")\n",
      "Workflow: Parallelization\n",
      "A task is broken down into independent subtasks that are processed simultaneously by multiple LLMs, with their outputs being aggregated. This pattern uses concurrency for tasks. The initial query (or parts of it) is sent to multiple LLMs in parallel with individual prompts/goals. Once all branches are complete, their individual results are collected and passed to a final aggregator LLM, which synthesizes them into the final response. This can improve latency if subtasks don't depend on each other, or enhance quality through techniques like majority voting or generating diverse options.\n",
      "Use Cases:\n",
      "RAG with query decomposition: Breaking a complex query into sub-queries, running retrievals for each in parallel, and synthesizing the results.\n",
      "Analyzing large documents: Dividing the document into sections, summarizing each section in parallel, and then combining the summaries.\n",
      "Generating multiple perspectives: Asking multiple LLMs the same question with different persona prompts and aggregating their responses.\n",
      "Map-reduce style operations on data.\n",
      "import\n",
      "os\n",
      "import\n",
      "asyncio\n",
      "import\n",
      "time\n",
      "from\n",
      "google\n",
      "import\n",
      "genai\n",
      "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
      "client\n",
      "=\n",
      "genai.Client(\n",
      "api_key\n",
      "=\n",
      "os.environ[\n",
      "\"GEMINI_API_KEY\"\n",
      "])\n",
      "async\n",
      "def\n",
      "generate_content\n",
      "(prompt:\n",
      "str\n",
      ") ->\n",
      "str\n",
      ":\n",
      "response\n",
      "=\n",
      "await\n",
      "client.aio.models.generate_content(\n",
      "model\n",
      "=\n",
      "\"gemini-2.0-flash\"\n",
      ",\n",
      "contents\n",
      "=\n",
      "prompt\n",
      ")\n",
      "return\n",
      "response.text.strip()\n",
      "async\n",
      "def\n",
      "parallel_tasks\n",
      "():\n",
      "# Define Parallel Tasks\n",
      "topic\n",
      "=\n",
      "\"a friendly robot exploring a jungle\"\n",
      "prompts\n",
      "=\n",
      "[\n",
      "f\n",
      "\"Write a short, adventurous story idea about\n",
      "{\n",
      "topic\n",
      "}\n",
      ".\"\n",
      ",\n",
      "f\n",
      "\"Write a short, funny story idea about\n",
      "{\n",
      "topic\n",
      "}\n",
      ".\"\n",
      ",\n",
      "f\n",
      "\"Write a short, mysterious story idea about\n",
      "{\n",
      "topic\n",
      "}\n",
      ".\"\n",
      "]\n",
      "# Run tasks concurrently and gather results\n",
      "start_time\n",
      "=\n",
      "time.time()\n",
      "tasks\n",
      "=\n",
      "[generate_content(prompt)\n",
      "for\n",
      "prompt\n",
      "in\n",
      "prompts]\n",
      "results\n",
      "=\n",
      "await\n",
      "asyncio.gather(\n",
      "*\n",
      "tasks)\n",
      "end_time\n",
      "=\n",
      "time.time()\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Time taken:\n",
      "{\n",
      "end_time\n",
      "-\n",
      "start_time\n",
      "}\n",
      "seconds\"\n",
      ")\n",
      "print\n",
      "(\n",
      "\"\n",
      "\\n\n",
      "--- Individual Results ---\"\n",
      ")\n",
      "for\n",
      "i, result\n",
      "in\n",
      "enumerate\n",
      "(results):\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Result\n",
      "{\n",
      "i\n",
      "+\n",
      "1}\n",
      ":\n",
      "{\n",
      "result\n",
      "}\\n\n",
      "\"\n",
      ")\n",
      "# Aggregate results and generate final story\n",
      "story_ideas\n",
      "=\n",
      "'\n",
      "\\n\n",
      "'\n",
      ".join([\n",
      "f\n",
      "\"Idea\n",
      "{\n",
      "i\n",
      "+\n",
      "1}\n",
      ":\n",
      "{\n",
      "result\n",
      "}\n",
      "\"\n",
      "for\n",
      "i, result\n",
      "in\n",
      "enumerate\n",
      "(results)])\n",
      "aggregation_prompt\n",
      "=\n",
      "f\n",
      "\"Combine the following three story ideas into a single, cohesive summary paragraph:\n",
      "{\n",
      "story_ideas\n",
      "}\n",
      "\"\n",
      "aggregation_response\n",
      "=\n",
      "await\n",
      "client.aio.models.generate_content(\n",
      "model\n",
      "=\n",
      "\"gemini-2.5-flash-preview-04-17\"\n",
      ",\n",
      "contents\n",
      "=\n",
      "aggregation_prompt\n",
      ")\n",
      "return\n",
      "aggregation_response.text\n",
      "result\n",
      "=\n",
      "await\n",
      "parallel_tasks()\n",
      "print\n",
      "(\n",
      "f\n",
      "\"\n",
      "\\n\n",
      "--- Aggregated Summary ---\n",
      "\\n{\n",
      "result\n",
      "}\n",
      "\"\n",
      ")\n",
      "Reflection Pattern\n",
      "An agent evaluates its own output and uses that feedback to refine its response iteratively. This pattern is also known as Evaluator-Optimizer and uses a self-correction loop. An initial LLM generates a response or completes a task. A second LLM step (or even the same LLM with a different prompt) then acts as a reflector or evaluator, critiquing the initial output against the requirements or desired quality. This critique (feedback) is then fed back, prompting the LLM to produce a refined output. This cycle can repeat until the evaluator confirms the requirements are met or a satisfing output is achieved.\n",
      "Use Cases:\n",
      "Code generation: Writing code, executing it, using error messages or test results as feedback to fix bugs.\n",
      "Writing and refinement: Generating a draft, reflecting on its clarity and tone, and then revising it.\n",
      "Complex problem solving: Generating a plan, evaluating its feasibility, and refining it based on the evaluation.\n",
      "Information retrieval: Searching for information and using an evaluator LLM to check if all required details were found before presenting the answer.\n",
      "import\n",
      "os\n",
      "import\n",
      "json\n",
      "from\n",
      "google\n",
      "import\n",
      "genai\n",
      "from\n",
      "pydantic\n",
      "import\n",
      "BaseModel\n",
      "import\n",
      "enum\n",
      "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
      "client\n",
      "=\n",
      "genai.Client(\n",
      "api_key\n",
      "=\n",
      "os.environ[\n",
      "\"GEMINI_API_KEY\"\n",
      "])\n",
      "class\n",
      "EvaluationStatus\n",
      "(\n",
      "enum\n",
      ".\n",
      "Enum\n",
      "):\n",
      "PASS\n",
      "=\n",
      "\"PASS\"\n",
      "FAIL\n",
      "=\n",
      "\"FAIL\"\n",
      "class\n",
      "Evaluation\n",
      "(\n",
      "BaseModel\n",
      "):\n",
      "evaluation: EvaluationStatus\n",
      "feedback:\n",
      "str\n",
      "reasoning:\n",
      "str\n",
      "# --- Initial Generation Function ---\n",
      "def\n",
      "generate_poem\n",
      "(topic:\n",
      "str\n",
      ", feedback:\n",
      "str\n",
      "=\n",
      "None\n",
      ") ->\n",
      "str\n",
      ":\n",
      "prompt\n",
      "=\n",
      "f\n",
      "\"Write a short, four-line poem about\n",
      "{\n",
      "topic\n",
      "}\n",
      ".\"\n",
      "if\n",
      "feedback:\n",
      "prompt\n",
      "+=\n",
      "f\n",
      "\"\n",
      "\\n\n",
      "Incorporate this feedback:\n",
      "{\n",
      "feedback\n",
      "}\n",
      "\"\n",
      "response\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.0-flash'\n",
      ",\n",
      "contents\n",
      "=\n",
      "prompt\n",
      ")\n",
      "poem\n",
      "=\n",
      "response.text.strip()\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Generated Poem:\n",
      "\\n{\n",
      "poem\n",
      "}\n",
      "\"\n",
      ")\n",
      "return\n",
      "poem\n",
      "# --- Evaluation Function ---\n",
      "def\n",
      "evaluate\n",
      "(poem:\n",
      "str\n",
      ") -> Evaluation:\n",
      "print\n",
      "(\n",
      "\"\n",
      "\\n\n",
      "--- Evaluating Poem ---\"\n",
      ")\n",
      "prompt_critique\n",
      "=\n",
      "f\n",
      "\"\"\"Critique the following poem. Does it rhyme well? Is it exactly four lines?\n",
      "Is it creative? Respond with PASS or FAIL and provide feedback.\n",
      "Poem:\n",
      "{\n",
      "poem\n",
      "}\n",
      "\"\"\"\n",
      "response_critique\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.0-flash'\n",
      ",\n",
      "contents\n",
      "=\n",
      "prompt_critique,\n",
      "config\n",
      "=\n",
      "{\n",
      "'response_mime_type'\n",
      ":\n",
      "'application/json'\n",
      ",\n",
      "'response_schema'\n",
      ": Evaluation,\n",
      "},\n",
      ")\n",
      "critique\n",
      "=\n",
      "response_critique.parsed\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Evaluation Status:\n",
      "{\n",
      "critique.evaluation\n",
      "}\n",
      "\"\n",
      ")\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Evaluation Feedback:\n",
      "{\n",
      "critique.feedback\n",
      "}\n",
      "\"\n",
      ")\n",
      "return\n",
      "critique\n",
      "# Reflection Loop\n",
      "max_iterations\n",
      "=\n",
      "3\n",
      "current_iteration\n",
      "=\n",
      "0\n",
      "topic\n",
      "=\n",
      "\"a robot learning to paint\"\n",
      "# simulated poem which will not pass the evaluation\n",
      "current_poem\n",
      "=\n",
      "\"With circuits humming, cold and bright,\n",
      "\\n\n",
      "A metal hand now holds a brush\"\n",
      "while\n",
      "current_iteration\n",
      "<\n",
      "max_iterations:\n",
      "current_iteration\n",
      "+=\n",
      "1\n",
      "print\n",
      "(\n",
      "f\n",
      "\"\n",
      "\\n\n",
      "--- Iteration\n",
      "{\n",
      "current_iteration\n",
      "}\n",
      "---\"\n",
      ")\n",
      "evaluation_result\n",
      "=\n",
      "evaluate(current_poem)\n",
      "if\n",
      "evaluation_result.evaluation\n",
      "==\n",
      "EvaluationStatus.\n",
      "PASS\n",
      ":\n",
      "print\n",
      "(\n",
      "\"\n",
      "\\n\n",
      "Final Poem:\"\n",
      ")\n",
      "print\n",
      "(current_poem)\n",
      "break\n",
      "else\n",
      ":\n",
      "current_poem\n",
      "=\n",
      "generate_poem(topic,\n",
      "feedback\n",
      "=\n",
      "evaluation_result.feedback)\n",
      "if\n",
      "current_iteration\n",
      "==\n",
      "max_iterations:\n",
      "print\n",
      "(\n",
      "\"\n",
      "\\n\n",
      "Max iterations reached. Last attempt:\"\n",
      ")\n",
      "print\n",
      "(current_poem)\n",
      "Tool Use Pattern\n",
      "LLM has the ability to invoke external functions or APIs to interact with the outside world, retrieve information, or perform actions. This pattern often referred to as Function Calling and is the most widely recognized pattern. The LLM is provided with definitions (name, description, input schema) of available tools (functions, APIs, databases, etc.). Based on the user query, the LLM can decide to call one or more tools by generating a structured output (like JSON) matching the required schema. This output is used to execute the actual external tool/function, and the result is returned to the LLM. The LLM then uses this result to formulate its final response to the user. This vastly extends the LLM's capabilities beyond its training data.\n",
      "Use Cases:\n",
      "Booking appointments using a calendar API.\n",
      "Retrieving real-time stock prices via a financial API.\n",
      "Searching a vector database for relevant documents (RAG).\n",
      "Controlling smart home devices.\n",
      "Executing code snippets.\n",
      "import\n",
      "os\n",
      "from\n",
      "google\n",
      "import\n",
      "genai\n",
      "from\n",
      "google.genai\n",
      "import\n",
      "types\n",
      "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
      "client\n",
      "=\n",
      "genai.Client(\n",
      "api_key\n",
      "=\n",
      "os.environ[\n",
      "\"GEMINI_API_KEY\"\n",
      "])\n",
      "# Define the function declaration for the model\n",
      "weather_function\n",
      "=\n",
      "{\n",
      "\"name\"\n",
      ":\n",
      "\"get_current_temperature\"\n",
      ",\n",
      "\"description\"\n",
      ":\n",
      "\"Gets the current temperature for a given location.\"\n",
      ",\n",
      "\"parameters\"\n",
      ": {\n",
      "\"type\"\n",
      ":\n",
      "\"object\"\n",
      ",\n",
      "\"properties\"\n",
      ": {\n",
      "\"location\"\n",
      ": {\n",
      "\"type\"\n",
      ":\n",
      "\"string\"\n",
      ",\n",
      "\"description\"\n",
      ":\n",
      "\"The city name, e.g. San Francisco\"\n",
      ",\n",
      "},\n",
      "},\n",
      "\"required\"\n",
      ": [\n",
      "\"location\"\n",
      "],\n",
      "},\n",
      "}\n",
      "# Placeholder function to simulate API call\n",
      "def\n",
      "get_current_temperature\n",
      "(location:\n",
      "str\n",
      ") ->\n",
      "dict\n",
      ":\n",
      "return\n",
      "{\n",
      "\"temperature\"\n",
      ":\n",
      "\"15\"\n",
      ",\n",
      "\"unit\"\n",
      ":\n",
      "\"Celsius\"\n",
      "}\n",
      "# Create the config object as shown in the user's example\n",
      "# Use client.models.generate_content with model, contents, and config\n",
      "tools\n",
      "=\n",
      "types.Tool(\n",
      "function_declarations\n",
      "=\n",
      "[weather_function])\n",
      "contents\n",
      "=\n",
      "[\n",
      "\"What's the temperature in London right now?\"\n",
      "]\n",
      "response\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.0-flash'\n",
      ",\n",
      "contents\n",
      "=\n",
      "contents,\n",
      "config\n",
      "=\n",
      "types.GenerateContentConfig(\n",
      "tools\n",
      "=\n",
      "[tools])\n",
      ")\n",
      "# Process the Response (Check for Function Call)\n",
      "response_part\n",
      "=\n",
      "response.candidates[\n",
      "0\n",
      "].content.parts[\n",
      "0\n",
      "]\n",
      "if\n",
      "response_part.function_call:\n",
      "function_call\n",
      "=\n",
      "response_part.function_call\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Function to call:\n",
      "{\n",
      "function_call.name\n",
      "}\n",
      "\"\n",
      ")\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Arguments:\n",
      "{dict\n",
      "(function_call.args)\n",
      "}\n",
      "\"\n",
      ")\n",
      "# Execute the Function\n",
      "if\n",
      "function_call.name\n",
      "==\n",
      "\"get_current_temperature\"\n",
      ":\n",
      "# Call the actual function\n",
      "api_result\n",
      "=\n",
      "get_current_temperature(\n",
      "*\n",
      "function_call.args)\n",
      "# Append function call and result of the function execution to contents\n",
      "follow_up_contents\n",
      "=\n",
      "[\n",
      "types.Part(\n",
      "function_call\n",
      "=\n",
      "function_call),\n",
      "types.Part.from_function_response(\n",
      "name\n",
      "=\n",
      "\"get_current_temperature\"\n",
      ",\n",
      "response\n",
      "=\n",
      "api_result\n",
      ")\n",
      "]\n",
      "# Generate final response\n",
      "response_final\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "\"gemini-2.0-flash\"\n",
      ",\n",
      "contents\n",
      "=\n",
      "contents\n",
      "+\n",
      "follow_up_contents,\n",
      "config\n",
      "=\n",
      "types.GenerateContentConfig(\n",
      "tools\n",
      "=\n",
      "[tools])\n",
      ")\n",
      "print\n",
      "(response_final.text)\n",
      "else\n",
      ":\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Error: Unknown function call requested:\n",
      "{\n",
      "function_call.name\n",
      "}\n",
      "\"\n",
      ")\n",
      "else\n",
      ":\n",
      "print\n",
      "(\n",
      "\"No function call found in the response.\"\n",
      ")\n",
      "print\n",
      "(response.text)\n",
      "Planning Pattern (Orchestrator-Workers)\n",
      "A central planner LLM breaks down a complex task into a dynamic list of subtasks, which are then delegated to specialized worker agents (often using Tool Use) for execution. This pattern tries to solve complex problems requiring multi-step reasoning by creating an intial Plan. This plan is dynamically generated based on the user input. Subtasks are then assigned to \"Worker\" agents that execute them, potentially in parallel if dependencies allow. An \"Orchestrator\" or \"Synthesizer\" LLM collects the results from the workers, reflects on whether the overall goal has been achieved, and either synthesizes the final output or potentially initiates a re-planning step if necessary. This reduces the cognitive load on any single LLM call, improves reasoning quality, minimizes errors, and allows for dynamic adaptation of the workflow. The key difference from Routing is that the Planner generates a\n",
      "multi-step plan\n",
      "rather than selecting a single next step.\n",
      "Use Cases:\n",
      "Complex software development tasks: Breaking down \"build a feature\" into planning, coding, testing, and documentation subtasks.\n",
      "Research and report generation: Planning steps like literature search, data extraction, analysis, and report writing.\n",
      "Multi-modal tasks: Planning steps involving image generation, text analysis, and data integration.\n",
      "Executing complex user requests like \"Plan a 3-day trip to Paris, book flights and a hotel within my budget.\"\n",
      "import\n",
      "os\n",
      "from\n",
      "google\n",
      "import\n",
      "genai\n",
      "from\n",
      "pydantic\n",
      "import\n",
      "BaseModel, Field\n",
      "from\n",
      "typing\n",
      "import\n",
      "List\n",
      "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
      "client\n",
      "=\n",
      "genai.Client(\n",
      "api_key\n",
      "=\n",
      "os.environ[\n",
      "\"GEMINI_API_KEY\"\n",
      "])\n",
      "# Define the Plan Schema\n",
      "class\n",
      "Task\n",
      "(\n",
      "BaseModel\n",
      "):\n",
      "task_id:\n",
      "int\n",
      "description:\n",
      "str\n",
      "assigned_to:\n",
      "str\n",
      "=\n",
      "Field(\n",
      "description\n",
      "=\n",
      "\"Which worker type should handle this? E.g., Researcher, Writer, Coder\"\n",
      ")\n",
      "class\n",
      "Plan\n",
      "(\n",
      "BaseModel\n",
      "):\n",
      "goal:\n",
      "str\n",
      "steps: List[Task]\n",
      "# Step 1: Generate the Plan (Planner LLM)\n",
      "user_goal\n",
      "=\n",
      "\"Write a short blog post about the benefits of AI agents.\"\n",
      "prompt_planner\n",
      "=\n",
      "f\n",
      "\"\"\"\n",
      "Create a step-by-step plan to achieve the following goal.\n",
      "Assign each step to a hypothetical worker type (Researcher, Writer).\n",
      "Goal:\n",
      "{\n",
      "user_goal\n",
      "}\n",
      "\"\"\"\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Goal:\n",
      "{\n",
      "user_goal\n",
      "}\n",
      "\"\n",
      ")\n",
      "print\n",
      "(\n",
      "\"Generating plan...\"\n",
      ")\n",
      "# Use a model capable of planning and structured output\n",
      "response_plan\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.5-pro-preview-03-25'\n",
      ",\n",
      "contents\n",
      "=\n",
      "prompt_planner,\n",
      "config\n",
      "=\n",
      "{\n",
      "'response_mime_type'\n",
      ":\n",
      "'application/json'\n",
      ",\n",
      "'response_schema'\n",
      ": Plan,\n",
      "},\n",
      ")\n",
      "# Step 2: Execute the Plan (Orchestrator/Workers - Omitted for brevity)\n",
      "for\n",
      "step\n",
      "in\n",
      "response_plan.parsed.steps:\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Step\n",
      "{\n",
      "step.task_id\n",
      "}\n",
      ":\n",
      "{\n",
      "step.description\n",
      "}\n",
      "(Assignee:\n",
      "{\n",
      "step.assigned_to\n",
      "}\n",
      ")\"\n",
      ")\n",
      "Multi-Agent Pattern\n",
      "Coordinator, Manager approach\n",
      "Swarm approach\n",
      "Multiple distinct agents each assigned a specific role, persona, or expertise collaborate to achieve a common goal. This pattern uses autonomous or semi-autonomous agents. Each agent might have a unique role (e.g., Project Manager, Coder, Tester, Critic), specialized knowledge, or access to specific tools. They interact and collaborate, often coordinated by a central \"coordinator\" or \"manager\" agent (like the PM in the diagram) or using handoff logic, where one agent passes the control to another agent.\n",
      "Use Cases:\n",
      "Simulating debates or brainstorming sessions with different AI personas.\n",
      "Complex software creation involving agents for planning, coding, testing, and deployment.\n",
      "Running virtual experiments or simulations with agents representing different actors.\n",
      "Collaborative writing or content creation processes.\n",
      "Note: The example below a simplified example on how to use the Multi-Agent pattern with handoff logic and structured output. I recommend to take a look at\n",
      "LangGraph Multi-Agent Swarm\n",
      "or\n",
      "Crew AI\n",
      "from\n",
      "google\n",
      "import\n",
      "genai\n",
      "from\n",
      "pydantic\n",
      "import\n",
      "BaseModel, Field\n",
      "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
      "client\n",
      "=\n",
      "genai.Client(\n",
      "api_key\n",
      "=\n",
      "os.environ[\n",
      "\"GEMINI_API_KEY\"\n",
      "])\n",
      "# Define Structured Output Schemas\n",
      "class\n",
      "Response\n",
      "(\n",
      "BaseModel\n",
      "):\n",
      "handoff:\n",
      "str\n",
      "=\n",
      "Field(\n",
      "default\n",
      "=\n",
      "\"\"\n",
      ",\n",
      "description\n",
      "=\n",
      "\"The name/role of the agent to hand off to. Available agents: 'Restaurant Agent', 'Hotel Agent'\"\n",
      ")\n",
      "message:\n",
      "str\n",
      "=\n",
      "Field(\n",
      "description\n",
      "=\n",
      "\"The response message to the user or context for the next agent\"\n",
      ")\n",
      "# Agent Function\n",
      "def\n",
      "run_agent\n",
      "(agent_name:\n",
      "str\n",
      ", system_prompt:\n",
      "str\n",
      ", prompt:\n",
      "str\n",
      ") -> Response:\n",
      "response\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.0-flash'\n",
      ",\n",
      "contents\n",
      "=\n",
      "prompt,\n",
      "config\n",
      "=\n",
      "{\n",
      "'system_instruction'\n",
      ":\n",
      "f\n",
      "'You are\n",
      "{\n",
      "agent_name\n",
      "}\n",
      ".\n",
      "{\n",
      "system_prompt\n",
      "}\n",
      "'\n",
      ",\n",
      "'response_mime_type'\n",
      ":\n",
      "'application/json'\n",
      ",\n",
      "'response_schema'\n",
      ": Response}\n",
      ")\n",
      "return\n",
      "response.parsed\n",
      "# Define System Prompts for the agents\n",
      "hotel_system_prompt\n",
      "=\n",
      "\"You are a Hotel Booking Agent. You ONLY handle hotel bookings. If the user asks about restaurants, flights, or anything else, respond with a short handoff message containing the original request and set the 'handoff' field to 'Restaurant Agent'. Otherwise, handle the hotel request and leave 'handoff' empty.\"\n",
      "restaurant_system_prompt\n",
      "=\n",
      "\"You are a Restaurant Booking Agent. You handle restaurant recommendations and bookings based on the user's request provided in the prompt.\"\n",
      "# Prompt to be about a restaurant\n",
      "initial_prompt\n",
      "=\n",
      "\"Can you book me a table at an Italian restaurant for 2 people tonight?\"\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Initial User Request:\n",
      "{\n",
      "initial_prompt\n",
      "}\n",
      "\"\n",
      ")\n",
      "# Run the first agent (Hotel Agent) to force handoff logic\n",
      "output\n",
      "=\n",
      "run_agent(\n",
      "\"Hotel Agent\"\n",
      ", hotel_system_prompt, initial_prompt)\n",
      "# simulate a user interaction to change the prompt and handoff\n",
      "if\n",
      "output.handoff\n",
      "==\n",
      "\"Restaurant Agent\"\n",
      ":\n",
      "print\n",
      "(\n",
      "\"Handoff Triggered: Hotel to Restaurant\"\n",
      ")\n",
      "output\n",
      "=\n",
      "run_agent(\n",
      "\"Restaurant Agent\"\n",
      ", restaurant_system_prompt, initial_prompt)\n",
      "elif\n",
      "output.handoff\n",
      "==\n",
      "\"Hotel Agent\"\n",
      ":\n",
      "print\n",
      "(\n",
      "\"Handoff Triggered: Restaurant to Hotel\"\n",
      ")\n",
      "output\n",
      "=\n",
      "run_agent(\n",
      "\"Hotel Agent\"\n",
      ", hotel_system_prompt, initial_prompt)\n",
      "print\n",
      "(output.message)\n",
      "Combining and Customizing These Patterns\n",
      "It's important to remember that these patterns aren't fixed rules but flexible building blocks. Real-world agentic systems often combine elements from multiple patterns. A Planning agent might use Tool Use, and its workers could employ Reflection. A Multi-Agent system might use Routing internally for task assignment.\n",
      "The key to success with any LLM application, especially complex agentic systems, is empirical evaluation. Define metrics, measure performance, identify bottlenecks or failure points, and iterate on your design. Resist to over-engineer.\n",
      "Acknowledgements\n",
      "This overview was created with the help of deep and manual research, drawing inspiration and information from several excellent resources, including:\n",
      "5 Agentic AI Design Patterns\n",
      "What are Agentic Workflows?\n",
      "Building effective agents\n",
      "How Agents Can Improve LLM Performance\n",
      "Agentic Design Patterns\n",
      "Agent Recipes\n",
      "LangGraph Agentic Concepts\n",
      "OpenAI Agents Python Examples\n",
      "Anthropic Cookbook\n",
      "Philipp Schmid\n",
      "©\n",
      "2025\n",
      "Imprint\n",
      "RSS Feed\n",
      "theme\n",
      "Mail\n",
      "Twitter\n",
      "LinkedIn\n",
      "GitHub\n"
     ]
    }
   ],
   "source": [
    "# Let's try one out. Change the website and add print statements to follow along.\n",
    "\n",
    "ed = Website(\"https://www.philschmid.de/agentic-pattern\")\n",
    "print(ed.title)\n",
    "print(ed.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a17ef43-5696-4041-b63d-0df3a7c2c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5160f6d-3380-4f8a-8ca2-396abcc366b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that writes a User Prompt that asks for summaries of websites:\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e0219dd-5263-4542-8359-d6bae71eab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are looking at a website titled Zero to One: Learning Agentic Patterns\n",
      "The contents of this website is as follows; please provide a short summary of this website in markdown. If it includes news or announcements, then summarize these too.\n",
      "\n",
      "Philschmid\n",
      "Search\n",
      "⌘\n",
      "k\n",
      "Blog\n",
      "Projects\n",
      "Newsletter\n",
      "About Me\n",
      "Toggle Menu\n",
      "Zero to One: Learning Agentic Patterns\n",
      "May 5, 2025\n",
      "16\n",
      "minute read\n",
      "View Code\n",
      "AI agents. Agentic AI. Agentic architectures. Agentic workflows. Agentic patterns. Agents are everywhere. But what exactly\n",
      "are\n",
      "they, and how do we build robust and effective agentic systems? While the term \"agent\" is used broadly, a key characteristic is their ability to dynamically plan and execute tasks, often leveraging external tools and memory to achieve complex goals.\n",
      "This post aims to explore common design patterns. Think of these patterns as blueprints or reusable templates for building AI applications. Understanding them provides a mental model for tackling complex problems and designing systems that are scalable, modular, and adaptable.\n",
      "We'll dive into several common patterns, differentiating between more structured\n",
      "workflows\n",
      "and more dynamic\n",
      "agentic patterns\n",
      ". Workflows typically follow predefined paths, while agents have more autonomy in deciding their course of action.\n",
      "Why Do (Agentic) Patterns Matter?\n",
      "Patterns provide a structured way to think and design systems.\n",
      "Patterns allow us to build and grow AI applications in complexity and adapt to changing requirements. Modular designs based on patterns are easier to modify and extend.\n",
      "Patterns help manage the complexity of coordinating multiple agents, tools, and workflows by offering proven, reusable templates. They promote best practices and shared understanding among developers.\n",
      "When (and When Not) to Use Agents?\n",
      "Before diving into patterns, it's crucial to consider\n",
      "when\n",
      "an agentic approach is truly necessary.\n",
      "Always seek the simplest solution first. If you know the exact steps required to solve a problem, a fixed workflow or even a simple script might be more efficient and reliable than a agent.\n",
      "Agentic systems often trade increased latency and computational cost for potentially better performance on complex, ambiguous, or dynamic tasks. Be sure the benefits outweigh these costs.\n",
      "Use\n",
      "workflows\n",
      "for predictability and consistency when dealing with well-defined tasks where the steps are known.\n",
      "Use\n",
      "agents\n",
      "when flexibility, adaptability, and model-driven decision-making are needed.\n",
      "Keep it Simple (Still): Even when building agentic systems, strive for the simplest effective design. Overly complex agent can become difficult to debug and manage.\n",
      "Agency introduces inherent unpredictability and potential errors. Agentic systems must incorporate robust error logging, exception handling, and retry mechanisms, allowing the system (or the underlying LLM) a chance to self-correct.\n",
      "Below, we'll explore 3 common workflow patterns and 4 agentic patterns. We'll illustrate each using pure API calls, without relying on specific frameworks like LangChain, LangGraph, LlamaIndex, or CrewAI, to focus on the core concepts.\n",
      "Pattern Overview\n",
      "We will cover the following patterns:\n",
      "Pattern Overview\n",
      "Workflow: Prompt Chaining\n",
      "Workflow: Routing or Handoff\n",
      "Workflow: Parallelization\n",
      "Reflection Pattern\n",
      "Tool Use Pattern\n",
      "Planning Pattern (Orchestrator-Workers)\n",
      "Multi-Agent Pattern\n",
      "Workflow: Prompt Chaining\n",
      "The output of one LLM call sequentially feeds into the input of the next LLM call. This pattern decomposes a task into a fixed sequence of steps. Each step is handled by an LLM call that processes the output from the preceding one. It's suitable for tasks that can be cleanly broken down into predictable, sequential subtasks.\n",
      "Use Cases:\n",
      "Generating a structured document: LLM 1 creates an outline, LLM 2 validates the outline against criteria, LLM 3 writes the content based on the validated outline.\n",
      "Multi-step data processing: Extracting information, transforming it, and then summarizing it.\n",
      "Generating newsletters based on curated inputs.\n",
      "import\n",
      "os\n",
      "from\n",
      "google\n",
      "import\n",
      "genai\n",
      "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
      "client\n",
      "=\n",
      "genai.Client(\n",
      "api_key\n",
      "=\n",
      "os.environ[\n",
      "\"GEMINI_API_KEY\"\n",
      "])\n",
      "# --- Step 1: Summarize Text ---\n",
      "original_text\n",
      "=\n",
      "\"Large language models are powerful AI systems trained on vast amounts of text data. They can generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\"\n",
      "prompt1\n",
      "=\n",
      "f\n",
      "\"Summarize the following text in one sentence:\n",
      "{\n",
      "original_text\n",
      "}\n",
      "\"\n",
      "# Use client.models.generate_content\n",
      "response1\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.0-flash'\n",
      ",\n",
      "contents\n",
      "=\n",
      "prompt1\n",
      ")\n",
      "summary\n",
      "=\n",
      "response1.text.strip()\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Summary:\n",
      "{\n",
      "summary\n",
      "}\n",
      "\"\n",
      ")\n",
      "# --- Step 2: Translate the Summary ---\n",
      "prompt2\n",
      "=\n",
      "f\n",
      "\"Translate the following summary into French, only return the translation, no other text:\n",
      "{\n",
      "summary\n",
      "}\n",
      "\"\n",
      "# Use client.models.generate_content\n",
      "response2\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.0-flash'\n",
      ",\n",
      "contents\n",
      "=\n",
      "prompt2\n",
      ")\n",
      "translation\n",
      "=\n",
      "response2.text.strip()\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Translation:\n",
      "{\n",
      "translation\n",
      "}\n",
      "\"\n",
      ")\n",
      "Workflow: Routing\n",
      "An initial LLM acts as a router, classifying the user's input and directing it to the most appropriate specialized task or LLM. This pattern implements a separation of concerns and allows for optimizing individual downstream tasks (using specialized prompts, different models, or specific tools) in isolation. It improves efficiency and potentially reduces costs by using smaller models for simpler tasks. When a task is routed, the selected agent \"takes over\" responsibility for completion.\n",
      "Use Cases:\n",
      "Customer support systems: Routing queries to agents specialized in billing, technical support, or product information.\n",
      "Tiered LLM usage: Routing simple queries to faster, cheaper models (like Llama 3.1 8B) and complex or unusual questions to more capable models (like Gemini 1.5 Pro).\n",
      "Content generation: Routing requests for blog posts, social media updates, or ad copy to different specialized prompts/models.\n",
      "import\n",
      "os\n",
      "import\n",
      "json\n",
      "from\n",
      "google\n",
      "import\n",
      "genai\n",
      "from\n",
      "pydantic\n",
      "import\n",
      "BaseModel\n",
      "import\n",
      "enum\n",
      "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
      "client\n",
      "=\n",
      "genai.Client(\n",
      "api_key\n",
      "=\n",
      "os.environ[\n",
      "\"GEMINI_API_KEY\"\n",
      "])\n",
      "# Define Routing Schema\n",
      "class\n",
      "Category\n",
      "(\n",
      "enum\n",
      ".\n",
      "Enum\n",
      "):\n",
      "WEATHER\n",
      "=\n",
      "\"weather\"\n",
      "SCIENCE\n",
      "=\n",
      "\"science\"\n",
      "UNKNOWN\n",
      "=\n",
      "\"unknown\"\n",
      "class\n",
      "RoutingDecision\n",
      "(\n",
      "BaseModel\n",
      "):\n",
      "category: Category\n",
      "reasoning:\n",
      "str\n",
      "# Step 1: Route the Query\n",
      "user_query\n",
      "=\n",
      "\"What's the weather like in Paris?\"\n",
      "# user_query = \"Explain quantum physics simply.\"\n",
      "# user_query = \"What is the capital of France?\"\n",
      "prompt_router\n",
      "=\n",
      "f\n",
      "\"\"\"\n",
      "Analyze the user query below and determine its category.\n",
      "Categories:\n",
      "- weather: For questions about weather conditions.\n",
      "- science: For questions about science.\n",
      "- unknown: If the category is unclear.\n",
      "Query:\n",
      "{\n",
      "user_query\n",
      "}\n",
      "\"\"\"\n",
      "# Use client.models.generate_content with config for structured output\n",
      "response_router\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.0-flash-lite'\n",
      ",\n",
      "contents\n",
      "=\n",
      "prompt_router,\n",
      "config\n",
      "=\n",
      "{\n",
      "'response_mime_type'\n",
      ":\n",
      "'application/json'\n",
      ",\n",
      "'response_schema'\n",
      ": RoutingDecision,\n",
      "},\n",
      ")\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Routing Decision: Category=\n",
      "{\n",
      "response_router.parsed.category\n",
      "}\n",
      ", Reasoning=\n",
      "{\n",
      "response_router.parsed.reasoning\n",
      "}\n",
      "\"\n",
      ")\n",
      "# Step 2: Handoff based on Routing\n",
      "final_response\n",
      "=\n",
      "\"\"\n",
      "if\n",
      "response_router.parsed.category\n",
      "==\n",
      "Category.\n",
      "WEATHER\n",
      ":\n",
      "weather_prompt\n",
      "=\n",
      "f\n",
      "\"Provide a brief weather forecast for the location mentioned in: '\n",
      "{\n",
      "user_query\n",
      "}\n",
      "'\"\n",
      "weather_response\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.0-flash'\n",
      ",\n",
      "contents\n",
      "=\n",
      "weather_prompt\n",
      ")\n",
      "final_response\n",
      "=\n",
      "weather_response.text\n",
      "elif\n",
      "response_router.parsed.category\n",
      "==\n",
      "Category.\n",
      "SCIENCE\n",
      ":\n",
      "science_response\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "\"gemini-2.5-flash-preview-04-17\"\n",
      ",\n",
      "contents\n",
      "=\n",
      "user_query\n",
      ")\n",
      "final_response\n",
      "=\n",
      "science_response.text\n",
      "else\n",
      ":\n",
      "unknown_response\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "\"gemini-2.0-flash-lite\"\n",
      ",\n",
      "contents\n",
      "=\n",
      "f\n",
      "\"The user query is:\n",
      "{\n",
      "prompt_router\n",
      "}\n",
      ", but could not be answered. Here is the reasoning:\n",
      "{\n",
      "response_router.parsed.reasoning\n",
      "}\n",
      ". Write a helpful response to the user for him to try again.\"\n",
      ")\n",
      "final_response\n",
      "=\n",
      "unknown_response.text\n",
      "print\n",
      "(\n",
      "f\n",
      "\"\n",
      "\\n\n",
      "Final Response:\n",
      "{\n",
      "final_response\n",
      "}\n",
      "\"\n",
      ")\n",
      "Workflow: Parallelization\n",
      "A task is broken down into independent subtasks that are processed simultaneously by multiple LLMs, with their outputs being aggregated. This pattern uses concurrency for tasks. The initial query (or parts of it) is sent to multiple LLMs in parallel with individual prompts/goals. Once all branches are complete, their individual results are collected and passed to a final aggregator LLM, which synthesizes them into the final response. This can improve latency if subtasks don't depend on each other, or enhance quality through techniques like majority voting or generating diverse options.\n",
      "Use Cases:\n",
      "RAG with query decomposition: Breaking a complex query into sub-queries, running retrievals for each in parallel, and synthesizing the results.\n",
      "Analyzing large documents: Dividing the document into sections, summarizing each section in parallel, and then combining the summaries.\n",
      "Generating multiple perspectives: Asking multiple LLMs the same question with different persona prompts and aggregating their responses.\n",
      "Map-reduce style operations on data.\n",
      "import\n",
      "os\n",
      "import\n",
      "asyncio\n",
      "import\n",
      "time\n",
      "from\n",
      "google\n",
      "import\n",
      "genai\n",
      "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
      "client\n",
      "=\n",
      "genai.Client(\n",
      "api_key\n",
      "=\n",
      "os.environ[\n",
      "\"GEMINI_API_KEY\"\n",
      "])\n",
      "async\n",
      "def\n",
      "generate_content\n",
      "(prompt:\n",
      "str\n",
      ") ->\n",
      "str\n",
      ":\n",
      "response\n",
      "=\n",
      "await\n",
      "client.aio.models.generate_content(\n",
      "model\n",
      "=\n",
      "\"gemini-2.0-flash\"\n",
      ",\n",
      "contents\n",
      "=\n",
      "prompt\n",
      ")\n",
      "return\n",
      "response.text.strip()\n",
      "async\n",
      "def\n",
      "parallel_tasks\n",
      "():\n",
      "# Define Parallel Tasks\n",
      "topic\n",
      "=\n",
      "\"a friendly robot exploring a jungle\"\n",
      "prompts\n",
      "=\n",
      "[\n",
      "f\n",
      "\"Write a short, adventurous story idea about\n",
      "{\n",
      "topic\n",
      "}\n",
      ".\"\n",
      ",\n",
      "f\n",
      "\"Write a short, funny story idea about\n",
      "{\n",
      "topic\n",
      "}\n",
      ".\"\n",
      ",\n",
      "f\n",
      "\"Write a short, mysterious story idea about\n",
      "{\n",
      "topic\n",
      "}\n",
      ".\"\n",
      "]\n",
      "# Run tasks concurrently and gather results\n",
      "start_time\n",
      "=\n",
      "time.time()\n",
      "tasks\n",
      "=\n",
      "[generate_content(prompt)\n",
      "for\n",
      "prompt\n",
      "in\n",
      "prompts]\n",
      "results\n",
      "=\n",
      "await\n",
      "asyncio.gather(\n",
      "*\n",
      "tasks)\n",
      "end_time\n",
      "=\n",
      "time.time()\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Time taken:\n",
      "{\n",
      "end_time\n",
      "-\n",
      "start_time\n",
      "}\n",
      "seconds\"\n",
      ")\n",
      "print\n",
      "(\n",
      "\"\n",
      "\\n\n",
      "--- Individual Results ---\"\n",
      ")\n",
      "for\n",
      "i, result\n",
      "in\n",
      "enumerate\n",
      "(results):\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Result\n",
      "{\n",
      "i\n",
      "+\n",
      "1}\n",
      ":\n",
      "{\n",
      "result\n",
      "}\\n\n",
      "\"\n",
      ")\n",
      "# Aggregate results and generate final story\n",
      "story_ideas\n",
      "=\n",
      "'\n",
      "\\n\n",
      "'\n",
      ".join([\n",
      "f\n",
      "\"Idea\n",
      "{\n",
      "i\n",
      "+\n",
      "1}\n",
      ":\n",
      "{\n",
      "result\n",
      "}\n",
      "\"\n",
      "for\n",
      "i, result\n",
      "in\n",
      "enumerate\n",
      "(results)])\n",
      "aggregation_prompt\n",
      "=\n",
      "f\n",
      "\"Combine the following three story ideas into a single, cohesive summary paragraph:\n",
      "{\n",
      "story_ideas\n",
      "}\n",
      "\"\n",
      "aggregation_response\n",
      "=\n",
      "await\n",
      "client.aio.models.generate_content(\n",
      "model\n",
      "=\n",
      "\"gemini-2.5-flash-preview-04-17\"\n",
      ",\n",
      "contents\n",
      "=\n",
      "aggregation_prompt\n",
      ")\n",
      "return\n",
      "aggregation_response.text\n",
      "result\n",
      "=\n",
      "await\n",
      "parallel_tasks()\n",
      "print\n",
      "(\n",
      "f\n",
      "\"\n",
      "\\n\n",
      "--- Aggregated Summary ---\n",
      "\\n{\n",
      "result\n",
      "}\n",
      "\"\n",
      ")\n",
      "Reflection Pattern\n",
      "An agent evaluates its own output and uses that feedback to refine its response iteratively. This pattern is also known as Evaluator-Optimizer and uses a self-correction loop. An initial LLM generates a response or completes a task. A second LLM step (or even the same LLM with a different prompt) then acts as a reflector or evaluator, critiquing the initial output against the requirements or desired quality. This critique (feedback) is then fed back, prompting the LLM to produce a refined output. This cycle can repeat until the evaluator confirms the requirements are met or a satisfing output is achieved.\n",
      "Use Cases:\n",
      "Code generation: Writing code, executing it, using error messages or test results as feedback to fix bugs.\n",
      "Writing and refinement: Generating a draft, reflecting on its clarity and tone, and then revising it.\n",
      "Complex problem solving: Generating a plan, evaluating its feasibility, and refining it based on the evaluation.\n",
      "Information retrieval: Searching for information and using an evaluator LLM to check if all required details were found before presenting the answer.\n",
      "import\n",
      "os\n",
      "import\n",
      "json\n",
      "from\n",
      "google\n",
      "import\n",
      "genai\n",
      "from\n",
      "pydantic\n",
      "import\n",
      "BaseModel\n",
      "import\n",
      "enum\n",
      "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
      "client\n",
      "=\n",
      "genai.Client(\n",
      "api_key\n",
      "=\n",
      "os.environ[\n",
      "\"GEMINI_API_KEY\"\n",
      "])\n",
      "class\n",
      "EvaluationStatus\n",
      "(\n",
      "enum\n",
      ".\n",
      "Enum\n",
      "):\n",
      "PASS\n",
      "=\n",
      "\"PASS\"\n",
      "FAIL\n",
      "=\n",
      "\"FAIL\"\n",
      "class\n",
      "Evaluation\n",
      "(\n",
      "BaseModel\n",
      "):\n",
      "evaluation: EvaluationStatus\n",
      "feedback:\n",
      "str\n",
      "reasoning:\n",
      "str\n",
      "# --- Initial Generation Function ---\n",
      "def\n",
      "generate_poem\n",
      "(topic:\n",
      "str\n",
      ", feedback:\n",
      "str\n",
      "=\n",
      "None\n",
      ") ->\n",
      "str\n",
      ":\n",
      "prompt\n",
      "=\n",
      "f\n",
      "\"Write a short, four-line poem about\n",
      "{\n",
      "topic\n",
      "}\n",
      ".\"\n",
      "if\n",
      "feedback:\n",
      "prompt\n",
      "+=\n",
      "f\n",
      "\"\n",
      "\\n\n",
      "Incorporate this feedback:\n",
      "{\n",
      "feedback\n",
      "}\n",
      "\"\n",
      "response\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.0-flash'\n",
      ",\n",
      "contents\n",
      "=\n",
      "prompt\n",
      ")\n",
      "poem\n",
      "=\n",
      "response.text.strip()\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Generated Poem:\n",
      "\\n{\n",
      "poem\n",
      "}\n",
      "\"\n",
      ")\n",
      "return\n",
      "poem\n",
      "# --- Evaluation Function ---\n",
      "def\n",
      "evaluate\n",
      "(poem:\n",
      "str\n",
      ") -> Evaluation:\n",
      "print\n",
      "(\n",
      "\"\n",
      "\\n\n",
      "--- Evaluating Poem ---\"\n",
      ")\n",
      "prompt_critique\n",
      "=\n",
      "f\n",
      "\"\"\"Critique the following poem. Does it rhyme well? Is it exactly four lines?\n",
      "Is it creative? Respond with PASS or FAIL and provide feedback.\n",
      "Poem:\n",
      "{\n",
      "poem\n",
      "}\n",
      "\"\"\"\n",
      "response_critique\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.0-flash'\n",
      ",\n",
      "contents\n",
      "=\n",
      "prompt_critique,\n",
      "config\n",
      "=\n",
      "{\n",
      "'response_mime_type'\n",
      ":\n",
      "'application/json'\n",
      ",\n",
      "'response_schema'\n",
      ": Evaluation,\n",
      "},\n",
      ")\n",
      "critique\n",
      "=\n",
      "response_critique.parsed\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Evaluation Status:\n",
      "{\n",
      "critique.evaluation\n",
      "}\n",
      "\"\n",
      ")\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Evaluation Feedback:\n",
      "{\n",
      "critique.feedback\n",
      "}\n",
      "\"\n",
      ")\n",
      "return\n",
      "critique\n",
      "# Reflection Loop\n",
      "max_iterations\n",
      "=\n",
      "3\n",
      "current_iteration\n",
      "=\n",
      "0\n",
      "topic\n",
      "=\n",
      "\"a robot learning to paint\"\n",
      "# simulated poem which will not pass the evaluation\n",
      "current_poem\n",
      "=\n",
      "\"With circuits humming, cold and bright,\n",
      "\\n\n",
      "A metal hand now holds a brush\"\n",
      "while\n",
      "current_iteration\n",
      "<\n",
      "max_iterations:\n",
      "current_iteration\n",
      "+=\n",
      "1\n",
      "print\n",
      "(\n",
      "f\n",
      "\"\n",
      "\\n\n",
      "--- Iteration\n",
      "{\n",
      "current_iteration\n",
      "}\n",
      "---\"\n",
      ")\n",
      "evaluation_result\n",
      "=\n",
      "evaluate(current_poem)\n",
      "if\n",
      "evaluation_result.evaluation\n",
      "==\n",
      "EvaluationStatus.\n",
      "PASS\n",
      ":\n",
      "print\n",
      "(\n",
      "\"\n",
      "\\n\n",
      "Final Poem:\"\n",
      ")\n",
      "print\n",
      "(current_poem)\n",
      "break\n",
      "else\n",
      ":\n",
      "current_poem\n",
      "=\n",
      "generate_poem(topic,\n",
      "feedback\n",
      "=\n",
      "evaluation_result.feedback)\n",
      "if\n",
      "current_iteration\n",
      "==\n",
      "max_iterations:\n",
      "print\n",
      "(\n",
      "\"\n",
      "\\n\n",
      "Max iterations reached. Last attempt:\"\n",
      ")\n",
      "print\n",
      "(current_poem)\n",
      "Tool Use Pattern\n",
      "LLM has the ability to invoke external functions or APIs to interact with the outside world, retrieve information, or perform actions. This pattern often referred to as Function Calling and is the most widely recognized pattern. The LLM is provided with definitions (name, description, input schema) of available tools (functions, APIs, databases, etc.). Based on the user query, the LLM can decide to call one or more tools by generating a structured output (like JSON) matching the required schema. This output is used to execute the actual external tool/function, and the result is returned to the LLM. The LLM then uses this result to formulate its final response to the user. This vastly extends the LLM's capabilities beyond its training data.\n",
      "Use Cases:\n",
      "Booking appointments using a calendar API.\n",
      "Retrieving real-time stock prices via a financial API.\n",
      "Searching a vector database for relevant documents (RAG).\n",
      "Controlling smart home devices.\n",
      "Executing code snippets.\n",
      "import\n",
      "os\n",
      "from\n",
      "google\n",
      "import\n",
      "genai\n",
      "from\n",
      "google.genai\n",
      "import\n",
      "types\n",
      "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
      "client\n",
      "=\n",
      "genai.Client(\n",
      "api_key\n",
      "=\n",
      "os.environ[\n",
      "\"GEMINI_API_KEY\"\n",
      "])\n",
      "# Define the function declaration for the model\n",
      "weather_function\n",
      "=\n",
      "{\n",
      "\"name\"\n",
      ":\n",
      "\"get_current_temperature\"\n",
      ",\n",
      "\"description\"\n",
      ":\n",
      "\"Gets the current temperature for a given location.\"\n",
      ",\n",
      "\"parameters\"\n",
      ": {\n",
      "\"type\"\n",
      ":\n",
      "\"object\"\n",
      ",\n",
      "\"properties\"\n",
      ": {\n",
      "\"location\"\n",
      ": {\n",
      "\"type\"\n",
      ":\n",
      "\"string\"\n",
      ",\n",
      "\"description\"\n",
      ":\n",
      "\"The city name, e.g. San Francisco\"\n",
      ",\n",
      "},\n",
      "},\n",
      "\"required\"\n",
      ": [\n",
      "\"location\"\n",
      "],\n",
      "},\n",
      "}\n",
      "# Placeholder function to simulate API call\n",
      "def\n",
      "get_current_temperature\n",
      "(location:\n",
      "str\n",
      ") ->\n",
      "dict\n",
      ":\n",
      "return\n",
      "{\n",
      "\"temperature\"\n",
      ":\n",
      "\"15\"\n",
      ",\n",
      "\"unit\"\n",
      ":\n",
      "\"Celsius\"\n",
      "}\n",
      "# Create the config object as shown in the user's example\n",
      "# Use client.models.generate_content with model, contents, and config\n",
      "tools\n",
      "=\n",
      "types.Tool(\n",
      "function_declarations\n",
      "=\n",
      "[weather_function])\n",
      "contents\n",
      "=\n",
      "[\n",
      "\"What's the temperature in London right now?\"\n",
      "]\n",
      "response\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.0-flash'\n",
      ",\n",
      "contents\n",
      "=\n",
      "contents,\n",
      "config\n",
      "=\n",
      "types.GenerateContentConfig(\n",
      "tools\n",
      "=\n",
      "[tools])\n",
      ")\n",
      "# Process the Response (Check for Function Call)\n",
      "response_part\n",
      "=\n",
      "response.candidates[\n",
      "0\n",
      "].content.parts[\n",
      "0\n",
      "]\n",
      "if\n",
      "response_part.function_call:\n",
      "function_call\n",
      "=\n",
      "response_part.function_call\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Function to call:\n",
      "{\n",
      "function_call.name\n",
      "}\n",
      "\"\n",
      ")\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Arguments:\n",
      "{dict\n",
      "(function_call.args)\n",
      "}\n",
      "\"\n",
      ")\n",
      "# Execute the Function\n",
      "if\n",
      "function_call.name\n",
      "==\n",
      "\"get_current_temperature\"\n",
      ":\n",
      "# Call the actual function\n",
      "api_result\n",
      "=\n",
      "get_current_temperature(\n",
      "*\n",
      "function_call.args)\n",
      "# Append function call and result of the function execution to contents\n",
      "follow_up_contents\n",
      "=\n",
      "[\n",
      "types.Part(\n",
      "function_call\n",
      "=\n",
      "function_call),\n",
      "types.Part.from_function_response(\n",
      "name\n",
      "=\n",
      "\"get_current_temperature\"\n",
      ",\n",
      "response\n",
      "=\n",
      "api_result\n",
      ")\n",
      "]\n",
      "# Generate final response\n",
      "response_final\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "\"gemini-2.0-flash\"\n",
      ",\n",
      "contents\n",
      "=\n",
      "contents\n",
      "+\n",
      "follow_up_contents,\n",
      "config\n",
      "=\n",
      "types.GenerateContentConfig(\n",
      "tools\n",
      "=\n",
      "[tools])\n",
      ")\n",
      "print\n",
      "(response_final.text)\n",
      "else\n",
      ":\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Error: Unknown function call requested:\n",
      "{\n",
      "function_call.name\n",
      "}\n",
      "\"\n",
      ")\n",
      "else\n",
      ":\n",
      "print\n",
      "(\n",
      "\"No function call found in the response.\"\n",
      ")\n",
      "print\n",
      "(response.text)\n",
      "Planning Pattern (Orchestrator-Workers)\n",
      "A central planner LLM breaks down a complex task into a dynamic list of subtasks, which are then delegated to specialized worker agents (often using Tool Use) for execution. This pattern tries to solve complex problems requiring multi-step reasoning by creating an intial Plan. This plan is dynamically generated based on the user input. Subtasks are then assigned to \"Worker\" agents that execute them, potentially in parallel if dependencies allow. An \"Orchestrator\" or \"Synthesizer\" LLM collects the results from the workers, reflects on whether the overall goal has been achieved, and either synthesizes the final output or potentially initiates a re-planning step if necessary. This reduces the cognitive load on any single LLM call, improves reasoning quality, minimizes errors, and allows for dynamic adaptation of the workflow. The key difference from Routing is that the Planner generates a\n",
      "multi-step plan\n",
      "rather than selecting a single next step.\n",
      "Use Cases:\n",
      "Complex software development tasks: Breaking down \"build a feature\" into planning, coding, testing, and documentation subtasks.\n",
      "Research and report generation: Planning steps like literature search, data extraction, analysis, and report writing.\n",
      "Multi-modal tasks: Planning steps involving image generation, text analysis, and data integration.\n",
      "Executing complex user requests like \"Plan a 3-day trip to Paris, book flights and a hotel within my budget.\"\n",
      "import\n",
      "os\n",
      "from\n",
      "google\n",
      "import\n",
      "genai\n",
      "from\n",
      "pydantic\n",
      "import\n",
      "BaseModel, Field\n",
      "from\n",
      "typing\n",
      "import\n",
      "List\n",
      "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
      "client\n",
      "=\n",
      "genai.Client(\n",
      "api_key\n",
      "=\n",
      "os.environ[\n",
      "\"GEMINI_API_KEY\"\n",
      "])\n",
      "# Define the Plan Schema\n",
      "class\n",
      "Task\n",
      "(\n",
      "BaseModel\n",
      "):\n",
      "task_id:\n",
      "int\n",
      "description:\n",
      "str\n",
      "assigned_to:\n",
      "str\n",
      "=\n",
      "Field(\n",
      "description\n",
      "=\n",
      "\"Which worker type should handle this? E.g., Researcher, Writer, Coder\"\n",
      ")\n",
      "class\n",
      "Plan\n",
      "(\n",
      "BaseModel\n",
      "):\n",
      "goal:\n",
      "str\n",
      "steps: List[Task]\n",
      "# Step 1: Generate the Plan (Planner LLM)\n",
      "user_goal\n",
      "=\n",
      "\"Write a short blog post about the benefits of AI agents.\"\n",
      "prompt_planner\n",
      "=\n",
      "f\n",
      "\"\"\"\n",
      "Create a step-by-step plan to achieve the following goal.\n",
      "Assign each step to a hypothetical worker type (Researcher, Writer).\n",
      "Goal:\n",
      "{\n",
      "user_goal\n",
      "}\n",
      "\"\"\"\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Goal:\n",
      "{\n",
      "user_goal\n",
      "}\n",
      "\"\n",
      ")\n",
      "print\n",
      "(\n",
      "\"Generating plan...\"\n",
      ")\n",
      "# Use a model capable of planning and structured output\n",
      "response_plan\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.5-pro-preview-03-25'\n",
      ",\n",
      "contents\n",
      "=\n",
      "prompt_planner,\n",
      "config\n",
      "=\n",
      "{\n",
      "'response_mime_type'\n",
      ":\n",
      "'application/json'\n",
      ",\n",
      "'response_schema'\n",
      ": Plan,\n",
      "},\n",
      ")\n",
      "# Step 2: Execute the Plan (Orchestrator/Workers - Omitted for brevity)\n",
      "for\n",
      "step\n",
      "in\n",
      "response_plan.parsed.steps:\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Step\n",
      "{\n",
      "step.task_id\n",
      "}\n",
      ":\n",
      "{\n",
      "step.description\n",
      "}\n",
      "(Assignee:\n",
      "{\n",
      "step.assigned_to\n",
      "}\n",
      ")\"\n",
      ")\n",
      "Multi-Agent Pattern\n",
      "Coordinator, Manager approach\n",
      "Swarm approach\n",
      "Multiple distinct agents each assigned a specific role, persona, or expertise collaborate to achieve a common goal. This pattern uses autonomous or semi-autonomous agents. Each agent might have a unique role (e.g., Project Manager, Coder, Tester, Critic), specialized knowledge, or access to specific tools. They interact and collaborate, often coordinated by a central \"coordinator\" or \"manager\" agent (like the PM in the diagram) or using handoff logic, where one agent passes the control to another agent.\n",
      "Use Cases:\n",
      "Simulating debates or brainstorming sessions with different AI personas.\n",
      "Complex software creation involving agents for planning, coding, testing, and deployment.\n",
      "Running virtual experiments or simulations with agents representing different actors.\n",
      "Collaborative writing or content creation processes.\n",
      "Note: The example below a simplified example on how to use the Multi-Agent pattern with handoff logic and structured output. I recommend to take a look at\n",
      "LangGraph Multi-Agent Swarm\n",
      "or\n",
      "Crew AI\n",
      "from\n",
      "google\n",
      "import\n",
      "genai\n",
      "from\n",
      "pydantic\n",
      "import\n",
      "BaseModel, Field\n",
      "# Configure the client (ensure GEMINI_API_KEY is set in your environment)\n",
      "client\n",
      "=\n",
      "genai.Client(\n",
      "api_key\n",
      "=\n",
      "os.environ[\n",
      "\"GEMINI_API_KEY\"\n",
      "])\n",
      "# Define Structured Output Schemas\n",
      "class\n",
      "Response\n",
      "(\n",
      "BaseModel\n",
      "):\n",
      "handoff:\n",
      "str\n",
      "=\n",
      "Field(\n",
      "default\n",
      "=\n",
      "\"\"\n",
      ",\n",
      "description\n",
      "=\n",
      "\"The name/role of the agent to hand off to. Available agents: 'Restaurant Agent', 'Hotel Agent'\"\n",
      ")\n",
      "message:\n",
      "str\n",
      "=\n",
      "Field(\n",
      "description\n",
      "=\n",
      "\"The response message to the user or context for the next agent\"\n",
      ")\n",
      "# Agent Function\n",
      "def\n",
      "run_agent\n",
      "(agent_name:\n",
      "str\n",
      ", system_prompt:\n",
      "str\n",
      ", prompt:\n",
      "str\n",
      ") -> Response:\n",
      "response\n",
      "=\n",
      "client.models.generate_content(\n",
      "model\n",
      "=\n",
      "'gemini-2.0-flash'\n",
      ",\n",
      "contents\n",
      "=\n",
      "prompt,\n",
      "config\n",
      "=\n",
      "{\n",
      "'system_instruction'\n",
      ":\n",
      "f\n",
      "'You are\n",
      "{\n",
      "agent_name\n",
      "}\n",
      ".\n",
      "{\n",
      "system_prompt\n",
      "}\n",
      "'\n",
      ",\n",
      "'response_mime_type'\n",
      ":\n",
      "'application/json'\n",
      ",\n",
      "'response_schema'\n",
      ": Response}\n",
      ")\n",
      "return\n",
      "response.parsed\n",
      "# Define System Prompts for the agents\n",
      "hotel_system_prompt\n",
      "=\n",
      "\"You are a Hotel Booking Agent. You ONLY handle hotel bookings. If the user asks about restaurants, flights, or anything else, respond with a short handoff message containing the original request and set the 'handoff' field to 'Restaurant Agent'. Otherwise, handle the hotel request and leave 'handoff' empty.\"\n",
      "restaurant_system_prompt\n",
      "=\n",
      "\"You are a Restaurant Booking Agent. You handle restaurant recommendations and bookings based on the user's request provided in the prompt.\"\n",
      "# Prompt to be about a restaurant\n",
      "initial_prompt\n",
      "=\n",
      "\"Can you book me a table at an Italian restaurant for 2 people tonight?\"\n",
      "print\n",
      "(\n",
      "f\n",
      "\"Initial User Request:\n",
      "{\n",
      "initial_prompt\n",
      "}\n",
      "\"\n",
      ")\n",
      "# Run the first agent (Hotel Agent) to force handoff logic\n",
      "output\n",
      "=\n",
      "run_agent(\n",
      "\"Hotel Agent\"\n",
      ", hotel_system_prompt, initial_prompt)\n",
      "# simulate a user interaction to change the prompt and handoff\n",
      "if\n",
      "output.handoff\n",
      "==\n",
      "\"Restaurant Agent\"\n",
      ":\n",
      "print\n",
      "(\n",
      "\"Handoff Triggered: Hotel to Restaurant\"\n",
      ")\n",
      "output\n",
      "=\n",
      "run_agent(\n",
      "\"Restaurant Agent\"\n",
      ", restaurant_system_prompt, initial_prompt)\n",
      "elif\n",
      "output.handoff\n",
      "==\n",
      "\"Hotel Agent\"\n",
      ":\n",
      "print\n",
      "(\n",
      "\"Handoff Triggered: Restaurant to Hotel\"\n",
      ")\n",
      "output\n",
      "=\n",
      "run_agent(\n",
      "\"Hotel Agent\"\n",
      ", hotel_system_prompt, initial_prompt)\n",
      "print\n",
      "(output.message)\n",
      "Combining and Customizing These Patterns\n",
      "It's important to remember that these patterns aren't fixed rules but flexible building blocks. Real-world agentic systems often combine elements from multiple patterns. A Planning agent might use Tool Use, and its workers could employ Reflection. A Multi-Agent system might use Routing internally for task assignment.\n",
      "The key to success with any LLM application, especially complex agentic systems, is empirical evaluation. Define metrics, measure performance, identify bottlenecks or failure points, and iterate on your design. Resist to over-engineer.\n",
      "Acknowledgements\n",
      "This overview was created with the help of deep and manual research, drawing inspiration and information from several excellent resources, including:\n",
      "5 Agentic AI Design Patterns\n",
      "What are Agentic Workflows?\n",
      "Building effective agents\n",
      "How Agents Can Improve LLM Performance\n",
      "Agentic Design Patterns\n",
      "Agent Recipes\n",
      "LangGraph Agentic Concepts\n",
      "OpenAI Agents Python Examples\n",
      "Anthropic Cookbook\n",
      "Philipp Schmid\n",
      "©\n",
      "2025\n",
      "Imprint\n",
      "RSS Feed\n",
      "theme\n",
      "Mail\n",
      "Twitter\n",
      "LinkedIn\n",
      "GitHub\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt_for(ed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aa92fd9-82e8-4fe1-9897-c7fe802bef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how this function creates exactly the format above\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69bd92b-0ec5-4364-bf71-185aac496d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this out, and then try for a few more websites\n",
    "\n",
    "messages_for(ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de5e6f-e023-42fc-9c21-866fb7f0a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559d827-d001-422e-a1ba-b141296421a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(\"https://www.philschmid.de/agentic-pattern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405cb6a8-fdff-4337-be02-fa0060189806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to display this nicely in the Jupyter output, using markdown\n",
    "\n",
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315c06c-aeba-4850-a66a-680af1608187",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(\"https://www.philschmid.de/agentic-pattern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe436b70-bbf0-4106-8e09-a1b6ff62cd55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
